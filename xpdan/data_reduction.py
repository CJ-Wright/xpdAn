#!/usr/bin/env python
##############################################################################
#
# xpdacq            by Billinge Group
#                   Simon J. L. Billinge sb2896@columbia.edu
#                   (c) 2016 trustees of Columbia University in the City of
#                        New York.
#                   All rights reserved
#
# File coded by:    Timothy Liu
#
# See AUTHORS.txt for a list of people who contributed.
# See LICENSE.txt for license information.
#
##############################################################################
#from dataportal import DataBroker as db
#from dataportal import get_events, get_table, get_images
#from metadatastore.commands import find_run_starts

import os
import warnings
import datetime
import numpy as np
import tifffile as tif
import matplotlib as plt
from time import strftime
from unittest.mock import MagicMock

from .glbl import an_glbl
from .utils import _clean_info, _timestampstr

# top definition for minimal impacts on the code 
if an_glbl._is_simulation:
    db = MagicMock()
    get_events = MagicMock()
    get_images = MagicMock()
else:
    from databroker.databroker import get_table
    from databroker.databroker import DataBroker as db
    from databroker import get_images
    from databroker import get_events

w_dir = os.path.join(an_glbl.home, 'tiff_base')
W_DIR = w_dir # in case of crashes in old codes



class DataReduction:
    """ class that handle operations on images from databroker header

        Note: not a callback
    """
    def __init__(self, image_field=None):
        # for file name
        self.fields = ['sa_name','sp_name', 'temperature']
        self.labels = ['dark_frame']
        self.root_dir_name = 'sa_name'
        if image_field is None:
            self.image_field = an_glbl.det_image_field

    def _feature_gen(self, event):
        ''' generate a human readable file name.

        file name is generated by metadata information in event
        run_start
        '''
        feature_list = []
        run_start = event.descriptor['run_start']
        uid = run_start['uid'][:6]
        # get special label
        for el in self.labels:
                label = run_start.get(el, None)
                if label is not None:
                    feature_list.append(str(label))
                else:
                    pass
        # get fields
        for key in self.fields:
            el = run_start.get(key, None)
            if el is not None:
                # truncate string length
                if len(el)>12:
                    value = el[:12]
                # clear space
                feature = _clean_info(el)
                feature_list.append(feature)
            else:
                pass
        # get uid
        feature_list.append(uid)
        return "_".join(feature_list)

    def pull_dark(self, header):
        dark_uid = header.start.get(an_glbl.dark_field_key, None)
        if dark_uid is None:
            print("INFO: no dark frame is associated in this header, "
                  "subrraction will not be processed")
            return None
        else:
            dark_search = {'group': 'XPD', 'uid': dark_uid}
            dark_header = db(**dark_search)
            dark_img = np.asarray(get_images(dark_header,
                                             self.image_field)).squeeze()
        return dark_img

    def _dark_sub(self, event, dark_img):
        """ priviate method operates on event level """
        dark_sub = False
        if dark_img is not None and isinstance(dark_img, np.ndarray):
            dark_sub = True
        img = event['data'][self.image_field]
        ind = event['seq_num']
        event_timestamp = event['timestamps'][self.image_field]
        # dark subtration logic
        if dark_img is not None:
            img -= dark_img
        return img, event_timestamp, ind, dark_sub

    def dark_sub(self, hedear):
        """ public method operates on header level """
        img_list = []
        timestamp_list = []
        dark_img = self.pull_dark(header)
        for ev in get_events(header):
            sub_img, event_timestamp = self._dark_sub(ev, dark_img)
            img_list.append(sub_img)
            timestamp_list.append(timestamp)
        return img_list, timestamp_list, dark_img, header.start

    def _file_name(self, event, event_timestamp, ind):
        """ priviate method operates on event level """
        f_name = self._feature_gen(event)
        f_name = '_'.join([f_name,
                           _timestampstr(event_timestamp)])
        f_name = '{}_{:04d}.tif'.format(f_name, ind)
        return f_name

    def save_tiff(self, headers, dark_sub=True, max_count=None,
                  dryrun=False):
        """ operate on header level """
        if type(list(headers)[1]) == str:
            header_list = list()
            header_list.append(headers)
        else:
            header_list = headers

        for header in header_list:
            # create root_dir
            root = header.start.get(self.root_dir_name, None)
            if root is not None:
                root_dir = os.path.join(W_DIR, root)
                os.makedirs(root_dir, exist_ok=True)
            else:
                root_dir = W_DIR
            # dark logic
            dark_img = self.pull_dark(header)
            if not dark_sub:
                dark_img = None # no sub
            # event
            for event in get_events(header, fill=True):
                img, event_timestamp, ind, dark_sub = self._dark_sub(event,
                                                                     dark_img)
                f_name = self._file_name(event, event_timestamp, ind)
                if dark_sub:
                    f_name = 'sub_' + f_name
                # save
                w_name = os.path.join(root_dir, f_name)
                if not dryrun:
                    tif.imsave(w_name, img)
                    if os.path.isfile(w_name):
                        print('image "%s" has been saved at "%s"' %
                            (f_name, root_dir))
                    else:
                        print('Sorry, something went wrong with your tif saving')
                        return
                else:
                    print("dryrun: image {} has been saved at {}"
                          .format(f_name, root_dir))
                if max_count is not None and ind >= max_count:
                    # break the loop if max_count reached, move to next header
                    break
        print('||********Saving process FINISHED********||')

# init
xpd_data_proc = DataReduction()

# back-support alias
def save_tiff(headers, dark_sub=True, max_count=None, dryrun=False):
    """ save images obtained from dataBroker as tiff format files.

    Parameters
    ----------
    headers : list
        a list of header objects obtained from a query to dataBroker

    dark_subtraction : bool, optional
        Default is True, which allows dark/background subtraction to 
        be done before saving each image. If header doesn't contain
        necessary information to perform dark subtraction, uncorrected
        image will be saved.

    max_count : int, optional
        The maximum number of events to process per-run.  This can be
        useful to 'preview' an export or if there are corrupted files
        in the data stream (ex from the IOC crashing during data
        acquisition).

    dryrun : bool, optional
        if set to True, file won't be saved. default is False
    """
    xpd_data_proc.save_tiff(headers, dark_sub, max_count, dryrun)

def save_last_tiff(dark_sub=True, max_count=None, dryrun=False):
    """ save images from the most recent scan as tiff format files.

    Parameters
    ----------
    dark_subtraction : bool, optional
        Default is True, which allows dark/background subtraction to 
        be done before saving each image. If header doesn't contain
        necessary information to perform dark subtraction, uncorrected
        image will be saved.

    max_count : int, optional
        The maximum number of events to process per-run.  This can be
        useful to 'preview' an export or if there are corrupted files
        in the data stream (ex from the IOC crashing during data acquisition).

    dryrun : bool, optional
        if set to True, file won't be saved. default is False
    """
    xpd_data_proc.save_tiff(db[-1], dark_sub, max_count, dryrun)

